import os
import sys
import re
import csv
import time
import signal
import asyncio
import aiohttp
from urllib.parse import urlparse, urlunparse, urljoin, parse_qs
from collections import defaultdict
from typing import Set, Tuple, Dict, Any, List
from concurrent.futures import ThreadPoolExecutor

from bs4 import BeautifulSoup
from pybloom_live import ScalableBloomFilter
from fake_useragent import UserAgent
import logging

# æ·±åº¦é€’å½’ä¿æŠ¤
sys.setrecursionlimit(10000)

# å†›å·¥çº§æ‰«æé…ç½®
CONFIG = {
    "max_depth": 3,
    "request_timeout": 35,
    "concurrency_range": (50, 200),
    "forbidden_ports": {22, 3306, 3389},
    "sensitive_ext": {
        'config', 'ini', 'env', 'zip', 'rar', '7z', 'tar', 'gz', 'bz2', 'xz',
        'bak', 'key', 'conf', 'properties', 'sql', 'db', 'dbf', 'pem', 'crt',
        'jks', 'p12', 'audit', 'dmg', 'iso', 'img', 'vmdk', 'apk', 'jar'
    },
    "sensitive_paths": [
        re.compile(r'/(backup|archive)/', re.I),
        re.compile(r'\.(git|svn)/', re.I)
    ],
    "ignore_ext": {'png', 'jpg', 'jpeg', 'gif'}
}

# ä¼ä¸šçº§æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler("scan_pro.log", mode='a', encoding='utf-8'),
        logging.StreamHandler()
    ]
)


class ScannerPro:
    def __init__(self):
        """åˆå§‹åŒ–æ‰«æå¼•æ“"""
        self.dedup_filter = ScalableBloomFilter(initial_capacity=100000, error_rate=0.001)
        self.ua = UserAgent()
        self.stats = defaultdict(int)
        self.findings = defaultdict(list)
        self.blocked_domains: Set[str] = set()
        self.active_tasks: Dict[str, Set[asyncio.Task]] = defaultdict(set)
        self.concurrency_ctrl = asyncio.Semaphore(CONFIG["concurrency_range"][1])
        self._shutdown = False
        self.session = None
        self.scanned_domains: Set[str] = set()  # æ–°å¢ï¼šç”¨äºè®°å½•å·²æ‰«æçš„åŸŸå
        signal.signal(signal.SIGINT, self._graceful_shutdown)

    async def _scan_worker(self, url: str, depth: int = 0):
        """é—ªç”µçº§å“åº”ç»ˆæ­¢çš„æ‰«æçº¿ç¨‹"""
        if self._shutdown:
            raise asyncio.CancelledError("ä¸»åŠ¨ç»ˆæ­¢")

        try:
            normalized_url = await self._normalize_url(url)
            parsed = urlparse(normalized_url)
            full_domain = parsed.netloc

            if full_domain in self.blocked_domains:
                return

            # æ–°å¢ï¼šè®°å½•å·²æ‰«æçš„åŸŸå
            self.scanned_domains.add(full_domain)

            is_sensitive, reason = self._is_sensitive(normalized_url)
            if is_sensitive:
                self.findings["critical"].append({
                    "url": normalized_url,
                    "reason": reason
                })
                self.blocked_domains.add(full_domain)
                logging.critical(f"ğŸš¨ å‘ç°æ•æ„Ÿæ–‡ä»¶é˜»æ–­åŸŸå [{full_domain}]")
                await self._cancel_domain_tasks(full_domain)
                return

            async with self.concurrency_ctrl:
                async with self.session.get(
                        normalized_url,
                        allow_redirects=False,
                        timeout=aiohttp.ClientTimeout(total=CONFIG["request_timeout"])
                ) as resp:
                    self.stats['total_requests'] += 1

                    if depth == 0 and 'text/html' in resp.headers.get('Content-Type', ''):
                        content = await resp.text()
                        soup = BeautifulSoup(content, 'lxml')
                        links = [urljoin(normalized_url, tag['href']) for tag in soup.select('a[href]')]
                        await self._schedule_tasks(links, depth + 1)

        except (aiohttp.ClientError, asyncio.CancelledError) as e:
            if isinstance(e, asyncio.CancelledError):
                raise  # ç›´æ¥é‡æ–°æŠ›å‡ºä¿è¯å¿«é€Ÿç»ˆæ­¢
            self.stats['failed_requests'] += 1
            logging.debug(f"è¯·æ±‚å¼‚å¸¸: {str(e)}")
        except Exception as e:
            self.stats['failed_requests'] += 1
            logging.error(f"æœªçŸ¥é”™è¯¯: {str(e)}")

    async def run(self, targets: list):
        """é—ªç”µçº§å“åº”è¿è¡Œå…¥å£"""
        try:
            async with aiohttp.ClientSession(
                    headers={"User-Agent": self.ua.random},
                    connector=aiohttp.TCPConnector(
                        ssl=False,
                        limit=200,
                        limit_per_host=20
                    )
            ) as self.session:
                monitor_task = asyncio.create_task(self._progress_monitor())
                main_task = asyncio.create_task(self._schedule_tasks(targets, 0))

                try:
                    await asyncio.wait_for(main_task, timeout=3600)
                except (asyncio.CancelledError, KeyboardInterrupt, asyncio.TimeoutError):
                    self._shutdown = True
                    logging.warning("æ­£åœ¨ç´§æ€¥ç»ˆæ­¢æ‰«æè¿›ç¨‹...")

                    # é—ªç”µçº§ç»ˆæ­¢ç­–ç•¥
                    all_tasks = {t for tasks in self.active_tasks.values() for t in tasks}
                    for task in all_tasks:
                        task.cancel()

                    # æé€Ÿç­‰å¾…ï¼ˆæœ€å¤š2ç§’ï¼‰
                    await asyncio.wait(
                        all_tasks,
                        timeout=min(2.0, len(all_tasks) * 0.01),
                        return_when=asyncio.ALL_COMPLETED
                    )
                finally:
                    monitor_task.cancel()
                    try:
                        await monitor_task
                    except asyncio.CancelledError:
                        pass

                    # æœ€ç»ˆæ¸…ç†
                    await self.session.close()

                # ç«‹å³ç”ŸæˆæŠ¥å‘Š
                report_file = await self.generate_report()
                print(f"\nğŸ”š æ‰«æç»ˆæ­¢ | æŠ¥å‘Šæ–‡ä»¶: {os.path.abspath(report_file)}")
                # æ–°å¢ï¼šè¾“å‡ºå·²æ‰«æçš„åŸŸåæ•°ç›®
                print(f"å·²æ‰«æçš„åŸŸåæ•°ç›®: {len(self.scanned_domains)}")
        except Exception as e:
            logging.error(f"Sessionå¼‚å¸¸: {str(e)}")
            raise

    def _graceful_shutdown(self, signum, frame):
        """ä¼˜é›…å…³é—­å¤„ç†"""
        logging.warning("æ¥æ”¶åˆ°ç»ˆæ­¢ä¿¡å·ï¼Œæ­£åœ¨ä¿å­˜æ‰«æçŠ¶æ€...")
        self._shutdown = True

    async def _normalize_url(self, raw_url: str) -> str:
        """URLæ ‡å‡†åŒ–å¤„ç†ï¼ˆå†›å·¥çº§ï¼‰"""
        url = raw_url.strip().lower()
        if not url.startswith(('http://', 'https://')):
            url = f'http://{url}'

        parsed = urlparse(url)
        if parsed.port in CONFIG["forbidden_ports"]:
            raise ValueError(f"ç¦æ­¢è®¿é—®é«˜å±ç«¯å£: {parsed.geturl()}")

        full_domain = parsed.netloc
        if full_domain in self.blocked_domains:
            raise ValueError(f"åŸŸåå·²è¢«é˜»æ–­: {full_domain}")

        # å‚æ•°å‡€åŒ–å¤„ç†
        query = parse_qs(parsed.query)
        clean_query = '&'.join(
            f"{k}={v[0]}" for k, v in query.items()
            if not k.startswith(('utm_', 'token', 'auth'))
        )
        return urlunparse((
            parsed.scheme, full_domain, parsed.path.rstrip('/'),
            parsed.params, clean_query, parsed.fragment
        ))

    def _is_sensitive(self, url: str) -> Tuple[bool, str]:
        """æ™ºèƒ½æ•æ„Ÿèµ„æºæ£€æµ‹"""
        parsed = urlparse(url)
        path = parsed.path.lower()

        # å¤šçº§æ‰©å±•åæ£€æµ‹
        if '.' in path:
            parts = path.split('.')
            combined_ext = '.'.join(parts[-2:])
            if combined_ext in {'tar.gz', 'tar.bz2', 'tar.xz'}:
                return True, f"å¤åˆå‹ç¼©æ ¼å¼: {combined_ext}"

        # æ‰©å±•åæ£€æµ‹
        if (ext := path.split('.')[-1]) in CONFIG["sensitive_ext"]:
            return True, f"æ•æ„Ÿæ‰©å±•å: {ext}"

        # è·¯å¾„æ­£åˆ™åŒ¹é…
        for pattern in CONFIG["sensitive_paths"]:
            if pattern.search(path):
                return True, f"è·¯å¾„åŒ¹é…: {pattern.pattern}"

        return False, None

    async def _schedule_tasks(self, urls: list, depth: int):
        """å¢å¼ºç‰ˆä»»åŠ¡è°ƒåº¦"""
        tasks = []
        for url in {u for u in urls if u}:
            parsed = urlparse(url)
            domain = parsed.netloc

            if domain in self.blocked_domains or url in self.dedup_filter:
                continue

            self.dedup_filter.add(url)
            task = asyncio.create_task(
                self._scan_worker(url, depth),
                name=f"ScanWorker:{domain}"
            )

            # æ·»åŠ å®‰å…¨å›è°ƒ
            def safe_remove(t):
                try:
                    self.active_tasks[domain].remove(t)
                except KeyError:
                    pass

            self.active_tasks[domain].add(task)
            task.add_done_callback(safe_remove)
            tasks.append(task)

        if tasks:
            try:
                await asyncio.wait_for(
                    asyncio.shield(asyncio.gather(*tasks, return_exceptions=True)),
                    timeout=CONFIG["request_timeout"] * 2
                )
            except (asyncio.TimeoutError, asyncio.CancelledError):
                pass

    async def _cancel_domain_tasks(self, domain: str):
        """åŸå­çº§ä»»åŠ¡ç»ˆæ­¢æ–¹æ¡ˆ"""
        if domain not in self.active_tasks:
            return

        cancel_tasks = self.active_tasks.pop(domain)
        if not cancel_tasks:
            return

        logging.info(f"ğŸ›‘ ç»ˆæ­¢ä»»åŠ¡ç»„ [{domain}] æ•°é‡:{len(cancel_tasks)}")

        # æ‰¹é‡é—ªç”µå–æ¶ˆ
        for task in cancel_tasks:
            if not task.done():
                task.cancel()

        # æé€Ÿç­‰å¾…ï¼ˆæœ€å¤š500msï¼‰
        done, pending = await asyncio.wait(
            cancel_tasks,
            timeout=min(0.5, len(cancel_tasks) * 0.001),
            return_when=asyncio.ALL_COMPLETED
        )

    def _emergency_cancel(self, task: asyncio.Task):
        """æ¯«ç§’çº§ä»»åŠ¡ç»ˆæ­¢"""
        try:
            task.cancel()
            if sys.platform == 'win32':
                with ThreadPoolExecutor(max_workers=1) as executor:
                    executor.submit(task.exception, timeout=0.1)
        except:
            pass

    async def _progress_monitor(self):
        """å®æ—¶æ€§èƒ½ç›‘æ§"""
        start = time.time()
        while not self._shutdown:
            elapsed = time.time() - start
            sys.stdout.write(
                f"\rğŸš€ æ‰«æä¸­ | æˆåŠŸ: {self.stats['total_requests']} | "
                f"é˜»æ–­: {len(self.blocked_domains)} | "
                f"é«˜å±: {len(self.findings['critical'])} | "
                f"è€—æ—¶: {elapsed:.1f}s"
            )
            sys.stdout.flush()
            await asyncio.sleep(0.5)

    async def generate_report(self):
        """ç”Ÿæˆç²¾ç®€æŠ¥å‘Š"""
        filename = f"å®‰å…¨å®¡è®¡æŠ¥å‘Š_{time.strftime('%Y%m%d_%H%M%S')}.csv"
        try:
            with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.writer(f)
                writer.writerow(["é£é™©ç­‰çº§", "URLåœ°å€", "æ£€æµ‹ä¾æ®"])
                for item in self.findings["critical"]:
                    writer.writerow(["ä¸¥é‡", item["url"], item["reason"]])
            logging.info(f"æŠ¥å‘Šè·¯å¾„: {os.path.abspath(filename)}")
            return filename
        except Exception as e:
            logging.error(f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}")
            return None

if __name__ == "__main__":
    targets = []
    try:
        if len(sys.argv) == 1:
            print("\nğŸ” index of/å®‰å…¨æ‰«æç³»ç»Ÿ v2.3")
            print("â”" * 40)
            input_file = input("è¯·è¾“å…¥ç›®æ ‡æ–‡ä»¶è·¯å¾„: ").strip(' "\'')
            if not os.path.exists(input_file):
                raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
            with open(input_file, encoding='utf-8') as f:
                targets = [ln.strip() for ln in f if ln.strip()]

        elif len(sys.argv) == 2:
            with open(sys.argv[1], encoding='utf-8') as f:
                targets = [ln.strip() for ln in f if ln.strip()]

        else:
            print("å‚æ•°é”™è¯¯")
            print("ç”¨æ³•: python scanner_pro.py [ç›®æ ‡æ–‡ä»¶]")
            sys.exit(1)

        engine = ScannerPro()
        asyncio.run(engine.run(targets))

    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except FileNotFoundError as e:
        print(f"\nâŒ æ–‡ä»¶é”™è¯¯: {str(e)}")
    except Exception as e:
        print(f"\nâ€¼ï¸ ç³»ç»Ÿå¼‚å¸¸: {str(e)}")
        logging.exception("è‡´å‘½é”™è¯¯")